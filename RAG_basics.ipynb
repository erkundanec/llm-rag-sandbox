{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfcb439",
   "metadata": {},
   "source": [
    "# Learning RAG by Building: A Hands-On Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee100717",
   "metadata": {},
   "source": [
    "What is RAG? (The Simple Explanation)The Restaurant AnalogyImagine you're a chef (the LLM) who can cook amazing dishes, but you only know recipes you learned in culinary school (training data). One day, a customer asks for a traditional Bengali fish recipe from 1850.Without RAG: You try to remember or improvise, possibly getting it wrong or admitting you don't know.With RAG: Before cooking, you:\n",
    "\n",
    "    1. Retrieve - Check your restaurant's cookbook library for relevant recipes\n",
    "    2. Augment - Read the specific recipe and understand the context\n",
    "    3. Generate - Cook the dish using both your skills AND the retrieved recipe\n",
    "\n",
    "RAG does exactly this for AI: it gives the model access to a \"library\" it can consult before answering.\n",
    "\n",
    "The Three Parts (Crystal Clear)\n",
    "## 1. Retrieval ğŸ”\n",
    "\n",
    "- Search a knowledge base for relevant information\n",
    "- Like using Ctrl+F, but smarter (semantic search)\n",
    "- Finds documents similar in meaning, not just matching words\n",
    "\n",
    "## 2. Augmentation ğŸ“\n",
    "\n",
    "- Take what you retrieved and add it to the prompt\n",
    "- \"Here's some context before my question...\"\n",
    "- The LLM now has fresh, relevant facts to work with\n",
    "\n",
    "## 3. Generation ğŸ’¬\n",
    "\n",
    "- The LLM produces an answer based on:\n",
    "\n",
    "    - Its training knowledge\n",
    "    - The retrieved context (the new part!)\n",
    "\n",
    "- More accurate, grounded in your specific data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d9d4a",
   "metadata": {},
   "source": [
    "## What is RAG? (Simple Explanation)\n",
    "\n",
    "### The Restaurant Analogy\n",
    "\n",
    "Imagine you're a chef (the LLM) who can cook amazing dishes, but you only know recipes you learned in culinary school years ago (training data). One day, a customer asks for a traditional Bengali fish recipe from 1850.\n",
    "\n",
    "**Without RAG:** You try to remember or improvise, possibly getting it wrong or saying \"I don't know.\"\n",
    "\n",
    "**With RAG:** Before cooking, you:\n",
    "\n",
    "- Retrieval ğŸ” - Check your restaurant's cookbook library for relevant recipes\n",
    "- Augmentation ğŸ“ - Read the specific recipe and add it to your working memory\n",
    "- Generation ğŸ’¬ - Cook the dish using both your culinary skills AND the retrieved recipe\n",
    "\n",
    "## Breaking Down the Three Steps\n",
    "\n",
    "### 1. Retrieval\n",
    "\n",
    "- Search your private knowledge base for information relevant to the query\n",
    "- Like Google search, but for your specific documents\n",
    "- Uses \"semantic search\" - finds documents similar in meaning, not just keyword matches\n",
    "\n",
    "### 2. Augmentation\n",
    "\n",
    "- Take the retrieved information and inject it into the prompt\n",
    "- \"Here's some context before I answer your question...\"\n",
    "- The LLM now has fresh, specific information to work with\n",
    "\n",
    "### 3. Generation\n",
    "\n",
    "- The LLM produces an answer using:\n",
    "  - Its general knowledge (from training)\n",
    "  - The retrieved context (your specific data)\n",
    "- Result: More accurate, grounded, and specific answers\n",
    "\n",
    "### Why Do We Need Retrieval?\n",
    "\n",
    "LLMs have two key limitations:\n",
    "\n",
    "1. Knowledge cutoff - They only know what they were trained on\n",
    "2. No access to private data - They can't read your company docs, personal notes, etc.\n",
    "\n",
    "RAG solves both by giving the LLM a \"library card\" to your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚     YOUR KNOWLEDGE BASE         â”‚\n",
    "                    â”‚  (5-10 text files about topics  â”‚\n",
    "                    â”‚   the LLM doesn't know)         â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  INDEXING (One-time setup)      â”‚\n",
    "                    â”‚                                 â”‚\n",
    "                    â”‚  1. Read documents              â”‚\n",
    "                    â”‚  2. Create embeddings (vectors) â”‚\n",
    "                    â”‚  3. Store in vector database    â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                                 â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚      VECTOR STORE               â”‚\n",
    "                    â”‚  [text chunk + embedding vector]â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚                    QUERY TIME                         â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                    User Query: \"What is an embedding?\"\n",
    "                                 â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  1. RETRIEVAL                   â”‚\n",
    "                    â”‚     - Convert query to vector   â”‚\n",
    "                    â”‚     - Search vector store       â”‚\n",
    "                    â”‚     - Find top-k similar chunks â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  2. AUGMENTATION                â”‚\n",
    "                    â”‚     - Build enriched prompt:    â”‚\n",
    "                    â”‚       \"Context: [chunks]        â”‚\n",
    "                    â”‚        Question: [query]\"       â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  3. GENERATION                  â”‚\n",
    "                    â”‚     - Call LLM with prompt      â”‚\n",
    "                    â”‚     - YOUR EXISTING API CLIENT! â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                                 â–¼\n",
    "                             Answer          '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099cde69",
   "metadata": {},
   "source": [
    "### Component Responsibilities\n",
    "\n",
    "Knowledge Base\n",
    "\n",
    "- Your private text files\n",
    "- What the LLM wasn't trained on\n",
    "- Could be company docs, research papers, personal notes\n",
    "\n",
    "Embedding Model\n",
    "\n",
    "- Converts text into vectors (lists of numbers)\n",
    "- Similar meanings â†’ similar vectors\n",
    "- Enables semantic search\n",
    "\n",
    "Vector Store\n",
    "\n",
    "- Database storing text chunks + their embeddings\n",
    "- Allows fast similarity search\n",
    "- We'll build a simple in-memory version\n",
    "\n",
    "Retriever\n",
    "\n",
    "- Takes a query\n",
    "- Finds most relevant chunks using vector similarity\n",
    "- Returns top-k results\n",
    "\n",
    "Your API Client\n",
    "\n",
    "- Unchanged! We reuse your existing code\n",
    "- Just receives richer prompts now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
