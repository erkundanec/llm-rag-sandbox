"""
Demo script showing RAG in action.
"""

from build_index import build_index
from rag_system import RAGSystem
import os


def main():
    print("\n" + "=" * 70)
    print("RAG TUTORIAL - INTERACTIVE DEMO")
    print("=" * 70)
    
    # Build index if needed
    if not os.path.exists("rag_store.pkl"):
        print("\nğŸ“š Building vector store (first time only)...")
        build_index()
    
    # Initialize RAG
    rag = RAGSystem()
    
    # Example queries
    examples = [
        "What are embeddings?",
        "How does semantic search work?",
        "What is the purpose of chunking?",
    ]
    
    print("\nğŸ¯ Running example comparisons...\n")
    
    for question in examples:
        rag.compare(question)
        input("\nPress Enter for next example...")
    
    # Interactive mode
    print("\n" + "=" * 70)
    print("Now try your own questions!")
    print("=" * 70)
    print("Commands:")
    print("  - Ask any question about embeddings, RAG, vector DBs, etc.")
    print("  - Type 'compare: <question>' to see with/without RAG")
    print("  - Type 'exit' to quit\n")
    
    while True:
        user_input = input("You: ").strip()
        if not user_input:
            continue
        if user_input.lower() == 'exit':
            print("Goodbye!")
            break
        
        if user_input.lower().startswith('compare:'):
            rag.compare(user_input[8:].strip())
        else:
            answer = rag.query(user_input)
            print(f"\nAnswer: {answer}\n")


if __name__ == "__main__":
    main()
```

### Step 7: Dependencies

Create **requirements.txt**:
```
requests>=2.31.0
numpy>=1.24.0
python-dotenv>=1.0.0
```

---

## 4. Complete Data Flow Explanation

Let me show you exactly how a query flows through the system:

### Example Query: "What are embeddings?"
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER INPUT: "What are embeddings?"                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: RETRIEVAL                                       â”‚
â”‚                                                         â”‚
â”‚ 1. Convert query to embedding                          â”‚
â”‚    "What are embeddings?"                              â”‚
â”‚    â†’ [0.123, -0.456, 0.789, ..., 1536 numbers]        â”‚
â”‚                                                         â”‚
â”‚ 2. Compare with all stored chunks (cosine similarity)  â”‚
â”‚    doc1_embeddings.txt:     0.87 âœ“âœ“âœ“                  â”‚
â”‚    doc4_semantic_search.txt: 0.65 âœ“                    â”‚
â”‚    doc3_rag.txt:            0.52 âœ“                     â”‚
â”‚    doc2_vector_db.txt:      0.48                       â”‚
â”‚    doc5_chunking.txt:       0.31                       â”‚
â”‚                                                         â”‚
â”‚ 3. Return top-3 chunks                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: AUGMENTATION                                    â”‚
â”‚                                                         â”‚
â”‚ Build enriched prompt:                                 â”‚
â”‚                                                         â”‚
â”‚ "Answer using ONLY the context below.                  â”‚
â”‚                                                         â”‚
â”‚  Context:                                              â”‚
â”‚  [Source 1: doc1_embeddings.txt]                       â”‚
â”‚  Embeddings are numerical representations of text...   â”‚
â”‚                                                         â”‚
â”‚  [Source 2: doc4_semantic_search.txt]                  â”‚
â”‚  Semantic search goes beyond keyword matching...       â”‚
â”‚                                                         â”‚
â”‚  [Source 3: doc3_rag.txt]                              â”‚
â”‚  Retrieval-Augmented Generation is a technique...      â”‚
â”‚                                                         â”‚
â”‚  Question: What are embeddings?                        â”‚
â”‚  Answer:"                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: GENERATION (YOUR EXISTING CODE!)                â”‚
â”‚                                                         â”‚
â”‚ call_llm(augmented_prompt)                             â”‚
â”‚   â†“                                                     â”‚
â”‚ LLM reads context + generates answer                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTPUT                                                  â”‚
â”‚                                                         â”‚
â”‚ "Embeddings are numerical representations of text that â”‚
â”‚  capture semantic meaning. Each piece of text is       â”‚
â”‚  converted into a vector with hundreds or thousands of â”‚
â”‚  dimensions. Similar texts have similar vectors,       â”‚
â”‚  measured by metrics like cosine similarity..."        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Where RAG Improves Over Plain LLM

**Without RAG:**
```
User: "What are embeddings?"
LLM: "Embeddings are... [generic explanation from training data]"
     - Might be outdated
     - Can't reference your specific docs
     - Might hallucinate details
```

**With RAG:**
```
User: "What are embeddings?"
[System retrieves doc1_embeddings.txt]
LLM: "According to your documentation, embeddings are numerical 
      representations that capture semantic meaning..."
     - Grounded in YOUR documents
     - Can cite sources
     - More accurate and relevant